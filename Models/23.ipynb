{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cpu' device\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "hier wird rnn mit class torch.nn.RNN implementiert\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device\")\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first= True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, future_steps = 1):\n",
    "        \"\"\"\n",
    "        Forward Pass\n",
    "        x: (batch_size, sequence_length, input_size)\n",
    "        future_steps: Number of future steps to predict\n",
    "        out: batch_size, sequence_length, hidden_size\n",
    "        hidden_state : num_layers, batch_size, hidden_size\n",
    "        \"\"\" \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        outputs = []\n",
    "        out, h = self.rnn(x, h0) # pass trough RNN\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) # initial output from last time step, Extrahiert den Hidden State des letzten Zeitschritts der Sequenz für jeden Batch.\n",
    "        outputs.append(out)\n",
    "        \n",
    "        for _ in range(future_steps - 1): # iterative prediction for future steps\n",
    "            out, h = self.rnn(out.unsqueeze(1), h)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            outputs.append(out)\n",
    "\n",
    "        return torch.stack(outputs, dim=1) # combine output : (batch_size, future_steps, num_classes)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, file_path, sequence_length, future_steps):\n",
    "        \"\"\"\n",
    "        Prepares data for sequence-based training with future steps targets\n",
    "        \"\"\"\n",
    "        \n",
    "        # Drop the time column (assume it's not needed for the model)\n",
    "        data = pd.read_csv(Path(r\"C:\\Users\\AleksandarMarjanovic\\Documents\\Modeling_Dynamic_Systems\\DynSys_and_DataSets\\lorenz_attractor_dataset.csv\"  )).iloc[1:, 1:].values  # Load X, Y, Z\n",
    "        self.sequence_length = sequence_length\n",
    "        self.future_steps = future_steps\n",
    "\n",
    "        self.X, self.Y = [], []\n",
    "       \n",
    "        for i in range(len(data) - sequence_length - future_steps + 1):\n",
    "            self.X.append(data[i:i+sequence_length])\n",
    "            self.Y.append(data[i+sequence_length: i+sequence_length+future_steps])     \n",
    "\n",
    "        # Create inputs (X) and targets (Y)\n",
    "        self.X = np.array(self.X)  # All rows except the last\n",
    "        self.Y = np.array(self.Y)   # All rows except the first (shifted)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return a single sample (input, target)\n",
    "        return torch.tensor(self.X[index], dtype=torch.float32), torch.tensor(self.Y[index], dtype=torch.float32)\n",
    "        \n",
    "\n",
    "def create_dataloader(file_path, batch_size, sequence_length, future_steps, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader from the given file path.\n",
    "    \"\"\"\n",
    "    dataset = TimeSeriesDataset(file_path, sequence_length, future_steps)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n",
      "=> Starting training\n",
      "=> Epoch: 1/10, Loss: 68.8327\n",
      "=> Epoch: 2/10, Loss: 64.6498\n",
      "=> Epoch: 3/10, Loss: 63.0428\n",
      "=> Epoch: 4/10, Loss: 63.0675\n",
      "=> Epoch: 5/10, Loss: 63.0555\n",
      "=> Epoch: 6/10, Loss: 63.0408\n",
      "=> Epoch: 7/10, Loss: 63.1026\n",
      "=> Epoch: 8/10, Loss: 63.0415\n",
      "=> Epoch: 9/10, Loss: 63.0324\n",
      "=> Epoch: 10/10, Loss: 63.0307\n",
      "Model saved to rnn_model_future_steps.pth\n",
      "training abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Function\n",
    "def train(model, data, epochs, optimizer, loss_fn, future_steps):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = {}\n",
    "    \n",
    "    print(\"=> Starting training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_losses = list()\n",
    "        \n",
    "        for X, Y in data:  # X, Y are batches\n",
    "            \n",
    "\n",
    "            # Send tensors to the device\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction = model(X, future_steps = future_steps)\n",
    "            loss = loss_fn(prediction, Y)\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "\n",
    "        # Store epoch loss\n",
    "        train_losses[epoch] = np.mean(epoch_losses)\n",
    "        print(f\"=> Epoch: {epoch + 1}/{epochs}, Loss: {train_losses[epoch]:.4f}\")\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "def save_model(model, file_name):\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "    print(f\"Model saved to {file_name}\")\n",
    "\n",
    "def load_model(model, file_name):\n",
    "    model.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {file_name}\")\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for X, Y in dataloader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            output, _ = model(X, model.init_zero_hidden(batch_size=X.shape[0]).to(device))\n",
    "\n",
    "            loss = loss_fn(output, Y)\n",
    "            total_loss += loss.item() * X.shape[0]  # Weighted by batch size\n",
    "            total_samples += X.shape[0]\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameter\n",
    "    file_path = Path(r\"C:\\Users\\AleksandarMarjanovic\\Documents\\Modeling_Dynamic_Systems\\DynSys_and_DataSets\\lorenz_attractor_dataset.csv\"  )\n",
    "\n",
    "    sequence_length = 20  # Eingabelänge (Anzahl Zeitpunkte)\n",
    "    future_steps = 10     # Anzahl der vorherzusagenden Schritte\n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "    hidden_size = 128\n",
    "    input_size = 3\n",
    "    output_size = 3\n",
    "    learning_rate = 0.01\n",
    "    # DataLoader mit Sequenzlänge und Zukunftsschritten\n",
    "    dataloader = create_dataloader(file_path, batch_size, sequence_length, future_steps)\n",
    "\n",
    "    # Modell initialisieren\n",
    "    model = RNN(input_size=input_size, hidden_size=hidden_size, num_layers=2, num_classes=output_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "    # Training\n",
    "    train_losses = train(model, dataloader, epochs, optimizer, loss_fn, future_steps=future_steps)\n",
    "    save_model(model, 'rnn_model_future_steps.pth')\n",
    "    print(\"training abgeschlossen\")\n",
    "    \n",
    "    \"\"\"save_model(model, 'rnn_model.pth')\n",
    "\n",
    "    # Modell nach dem Training laden und evaluieren\n",
    "    loaded_model = RNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "    loaded_model = load_model(loaded_model, 'rnn_model.pth')\n",
    "\n",
    "    # Testen und Evaluieren (optional, ein Testdatensatz muss vorhanden sein)\n",
    "    test_file_path = 'D:/Master_EI/FP/Modeling_Dynamic_Systems/DynSys_and_DataSets/lorenz_attractor_dataset_test.csv'  # Beispiel Testdatensatz\n",
    "    test_dataloader = create_dataloader(test_file_path, batch_size)\n",
    "    evaluate(loaded_model, test_dataloader, loss_fn)\n",
    "\n",
    "    print(\"Training abgeschlossen und Modell evaluiert.\")\"\"\"\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo\n"
     ]
    }
   ],
   "source": [
    "print(\"Hallo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
