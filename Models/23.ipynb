{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cpu' device\n",
      "=> Starting training\n",
      "=> Epoch: 1/30, Loss: 67.44285583496094\n",
      "=> Epoch: 2/30, Loss: 20.3575439453125\n",
      "=> Epoch: 3/30, Loss: 15.087815284729004\n",
      "=> Epoch: 4/30, Loss: 9.890474319458008\n",
      "=> Epoch: 5/30, Loss: 5.381691932678223\n",
      "=> Epoch: 6/30, Loss: 2.555542469024658\n",
      "=> Epoch: 7/30, Loss: 1.307839035987854\n",
      "=> Epoch: 8/30, Loss: 0.9152888059616089\n",
      "=> Epoch: 9/30, Loss: 0.7053211331367493\n",
      "=> Epoch: 10/30, Loss: 0.6069918870925903\n",
      "=> Epoch: 11/30, Loss: 0.5540006756782532\n",
      "=> Epoch: 12/30, Loss: 0.49745607376098633\n",
      "=> Epoch: 13/30, Loss: 0.5153460502624512\n",
      "=> Epoch: 14/30, Loss: 0.4463564157485962\n",
      "=> Epoch: 15/30, Loss: 0.44174879789352417\n",
      "=> Epoch: 16/30, Loss: 0.3940175473690033\n",
      "=> Epoch: 17/30, Loss: 0.3542339503765106\n",
      "=> Epoch: 18/30, Loss: 0.3496111333370209\n",
      "=> Epoch: 19/30, Loss: 0.35322168469429016\n",
      "=> Epoch: 20/30, Loss: 0.3436886668205261\n",
      "=> Epoch: 21/30, Loss: 0.332689493894577\n",
      "=> Epoch: 22/30, Loss: 0.29688432812690735\n",
      "=> Epoch: 23/30, Loss: 0.282344251871109\n",
      "=> Epoch: 24/30, Loss: 0.24657104909420013\n",
      "=> Epoch: 25/30, Loss: 0.26517534255981445\n",
      "=> Epoch: 26/30, Loss: 0.27448540925979614\n",
      "=> Epoch: 27/30, Loss: 0.3187588155269623\n",
      "=> Epoch: 28/30, Loss: 0.25498124957084656\n",
      "=> Epoch: 29/30, Loss: 0.3032912611961365\n",
      "=> Epoch: 30/30, Loss: 0.2993861138820648\n",
      "Model saved to rnn_model.pth\n",
      "Model loaded from rnn_model.pth\n",
      "Test Loss: 0.5628\n",
      "Training abgeschlossen und Modell evaluiert.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RNN character generator\n",
    "RNN implementation with Dense layers\n",
    "There is an RNN layer in pytorch, but in this case we will be using\n",
    "normal Dense layers to demonstrate the difference between\n",
    "RNN and Normal feedforward networks.\n",
    "This is a character level generator, which means it will create character by character\n",
    "You can input any text file and it will generate characters based on that text\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device\")\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic RNN block. This represents a single layer of RNN\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes) -> None:\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first= True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        x = self.i2h(x)\n",
    "        hidden_state = self.h2h(hidden_state)\n",
    "        hidden_state = torch.tanh(hidden_state + x)\n",
    "        out = self.h2o(hidden_state)\n",
    "        return out, hidden_state\n",
    "    \n",
    "    def init_zero_hidden(self, batch_size = 1) -> torch.Tensor:\n",
    "        \"\"\" \n",
    "        Helper function.\n",
    "        Returns a hidden state with specified batch size. Defaults to 1\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.zeros(batch_size, self.hidden_size, requires_grad = False)\n",
    "    \n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Reads CSV file and prepares data for batching.\n",
    "        Assumes the CSV has columns: Time, X, Y, Z.\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop the time column (assume it's not needed for the model)\n",
    "        self.data = data.iloc[1:, 1:].values  # Extract X, Y, Z\n",
    "        \n",
    "        # Create inputs (X) and targets (Y)\n",
    "        self.X = self.data[:-1]  # All rows except the last\n",
    "        self.Y = self.data[1:]   # All rows except the first (shifted)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return a single sample (input, target)\n",
    "        return torch.tensor(self.X[index], dtype=torch.float32), torch.tensor(self.Y[index], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def create_dataloader(file_path, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader from the given file path.\n",
    "    \"\"\"\n",
    "    dataset = TimeSeriesDataset(file_path)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "\n",
    "\n",
    "# Train Function\n",
    "def train(model: nn.Module, data: DataLoader, epochs: int, optimizer: optim.Optimizer, loss_fn: nn.Module):\n",
    "    \"\"\"\n",
    "    Trains the model for the specified number of epochs\n",
    "    Input\n",
    "    -----\n",
    "    model: RNN model to train\n",
    "    data: Iterable DataLoader\n",
    "    optimizer: Optimizer to use for each epoch\n",
    "    loss_fn: Function to calculate loss\n",
    "    \"\"\"\n",
    "    train_losses = {}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    print(\"=> Starting training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_losses = list()\n",
    "        \n",
    "        for X, Y in data:  # X, Y are batches\n",
    "            # Skip last batch if it doesn't match the batch_size\n",
    "            if X.shape[0] != model.batch_size:\n",
    "                continue\n",
    "\n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_zero_hidden(batch_size=model.batch_size)\n",
    "\n",
    "            # Send tensors to the device\n",
    "            X, Y, hidden = X.to(device), Y.to(device), hidden.to(device)\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output, hidden = model(X, hidden)\n",
    "            loss = loss_fn(output, Y)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to avoid vanishing/exploding gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=3)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record batch loss\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        # Store epoch loss\n",
    "        train_losses[epoch] = torch.tensor(epoch_losses).mean().item()\n",
    "        print(f\"=> Epoch: {epoch + 1}/{epochs}, Loss: {train_losses[epoch]}\")\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "def save_model(model, file_name):\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "    print(f\"Model saved to {file_name}\")\n",
    "\n",
    "def load_model(model, file_name):\n",
    "    model.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {file_name}\")\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for X, Y in dataloader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            output, _ = model(X, model.init_zero_hidden(batch_size=X.shape[0]).to(device))\n",
    "\n",
    "            loss = loss_fn(output, Y)\n",
    "            total_loss += loss.item() * X.shape[0]  # Weighted by batch size\n",
    "            total_samples += X.shape[0]\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameter\n",
    "    file_path = \"D:\\Master_EI\\FP\\Modeling_Dynamic_Systems\\DynSys_and_DataSets\\lorenz_attractor_dataset.csv\"  # Pfad zur CSV-Datei\n",
    "    batch_size = 32\n",
    "    input_size = 3  # X, Y, Z (3 Features)\n",
    "    hidden_size = 256\n",
    "    output_size = 3  # X, Y, Z (3 Features)\n",
    "    learning_rate = 0.001\n",
    "    epochs = 30\n",
    "\n",
    "    # DataLoader erstellen\n",
    "    dataloader = create_dataloader(file_path, batch_size)\n",
    "\n",
    "    # Modell, Optimizer und Verlustfunktion initialisieren\n",
    "    model = RNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Modell trainieren\n",
    "    train_losses = train(model, dataloader, epochs, optimizer, loss_fn)\n",
    "      \n",
    "    \n",
    "    save_model(model, 'rnn_model.pth')\n",
    "\n",
    "    # Modell nach dem Training laden und evaluieren\n",
    "    loaded_model = RNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "    loaded_model = load_model(loaded_model, 'rnn_model.pth')\n",
    "\n",
    "    # Testen und Evaluieren (optional, ein Testdatensatz muss vorhanden sein)\n",
    "    test_file_path = 'D:/Master_EI/FP/Modeling_Dynamic_Systems/DynSys_and_DataSets/lorenz_attractor_dataset_test.csv'  # Beispiel Testdatensatz\n",
    "    test_dataloader = create_dataloader(test_file_path, batch_size)\n",
    "    evaluate(loaded_model, test_dataloader, loss_fn)\n",
    "\n",
    "    print(\"Training abgeschlossen und Modell evaluiert.\")\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
