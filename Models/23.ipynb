{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "hier wird rnn mit class torch.nn.RNN implementiert\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device\")\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first= True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, future_steps = 1):\n",
    "        \"\"\"\n",
    "        Forward Pass\n",
    "        x: (batch_size, sequence_length, input_size)\n",
    "        future_steps: Number of future steps to predict\n",
    "        out: batch_size, sequence_length, hidden_size\n",
    "        hidden_state : num_layers, batch_size, hidden_size\n",
    "        \"\"\" \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        outputs = []\n",
    "        out, h = self.rnn(x, h0) # pass trough RNN\n",
    "\n",
    "        out = self.fc(out[:, -1, :]) # initial output from last time step, Extrahiert den Hidden State des letzten Zeitschritts der Sequenz fÃ¼r jeden Batch.\n",
    "        outputs.append(out)\n",
    "        \n",
    "        for _ in range(future_steps - 1): # iterative prediction for future steps\n",
    "            out, h = self.rnn(out.unsqueeze(1), h)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            outputs.append(out)\n",
    "\n",
    "        return torch.stack(outputs, dim=1) # combine output : (batch_size, future_steps, num_classes)\n",
    "     \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, file_path, sequence_length, future_steps):\n",
    "        \"\"\"\n",
    "        Prepares data for sequence-based training with future steps targets\n",
    "        \"\"\"\n",
    "        \n",
    "        # Drop the time column (assume it's not needed for the model)\n",
    "        data = pd.read_csv(file_path).iloc[1:, 1:].values  # Load X, Y, Z\n",
    "        self.sequence_length = sequence_length\n",
    "        self.future_steps = future_steps\n",
    "\n",
    "        self.X, self.Y = [], []\n",
    "       \n",
    "        for i in range(len(data) - sequence_length - future_steps + 1):\n",
    "            self.X.append(data[i:i+sequence_length])\n",
    "            self.Y.append(data[i+sequence_length: i+sequence_length+future_steps])     \n",
    "\n",
    "        # Create inputs (X) and targets (Y)\n",
    "        self.X = np.array(self.X)  # All rows except the last\n",
    "        self.Y = np.array(self.Y)   # All rows except the first (shifted)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return a single sample (input, target)\n",
    "        return torch.tensor(self.X[index], dtype=torch.float32), torch.tensor(self.Y[index], dtype=torch.float32)\n",
    "        \n",
    "\n",
    "def create_dataloader(file_path, batch_size, sequence_length, future_steps, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader from the given file path.\n",
    "    \"\"\"\n",
    "    dataset = TimeSeriesDataset(file_path, sequence_length, future_steps)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m create_dataloader(file_path, batch_size, sequence_length, future_steps)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Modell initialisieren\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     88\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "File \u001b[0;32m~/projects/Modeling_Dynamic_Systems/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/Modeling_Dynamic_Systems/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/Modeling_Dynamic_Systems/.venv/lib64/python3.9/site-packages/torch/nn/modules/rnn.py:283\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 283\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[0;32m~/projects/Modeling_Dynamic_Systems/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/Modeling_Dynamic_Systems/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Function\n",
    "def train(model, data, epochs, optimizer, loss_fn, future_steps):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = {}\n",
    "    \n",
    "    print(\"=> Starting training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_losses = list()\n",
    "        \n",
    "        for X, Y in data:  # X, Y are batches\n",
    "            \n",
    "\n",
    "            # Send tensors to the device\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction = model(X, future_steps = future_steps)\n",
    "            loss = loss_fn(prediction, Y)\n",
    "            \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "\n",
    "        # Store epoch loss\n",
    "        train_losses[epoch] = np.mean(epoch_losses)\n",
    "        print(f\"=> Epoch: {epoch + 1}/{epochs}, Loss: {train_losses[epoch]:.4f}\")\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "def save_model(model, file_name):\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "    print(f\"Model saved to {file_name}\")\n",
    "\n",
    "def load_model(model, file_name):\n",
    "    model.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {file_name}\")\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for X, Y in dataloader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "            output, _ = model(X, model.init_zero_hidden(batch_size=X.shape[0]).to(device))\n",
    "\n",
    "            loss = loss_fn(output, Y)\n",
    "            total_loss += loss.item() * X.shape[0]  # Weighted by batch size\n",
    "            total_samples += X.shape[0]\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameter\n",
    "    file_path = \"/home_net/ge36xax/projects/Modeling_Dynamic_Systems/DynSys_and_DataSets/lorenz_attractor_dataset.csv\"\n",
    "\n",
    "    sequence_length = 20  # EingabelÃ¤nge (Anzahl Zeitpunkte)\n",
    "    future_steps = 10     # Anzahl der vorherzusagenden Schritte\n",
    "    batch_size = 196\n",
    "    epochs = 300\n",
    "    hidden_size = 16\n",
    "    input_size = 3\n",
    "    output_size = 3\n",
    "    learning_rate = 0.001\n",
    "    # DataLoader mit SequenzlÃ¤nge und Zukunftsschritten\n",
    "    dataloader = create_dataloader(file_path, batch_size, sequence_length, future_steps)\n",
    "\n",
    "    # Modell initialisieren\n",
    "    model = RNN(input_size=input_size, hidden_size=hidden_size, num_layers=2, num_classes=output_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "    # Training\n",
    "    train_losses = train(model, dataloader, epochs, optimizer, loss_fn, future_steps=future_steps)\n",
    "    save_model(model, 'rnn_model_future_steps.pth')\n",
    "    print(\"training abgeschlossen\")\n",
    "    \n",
    "    \"\"\"save_model(model, 'rnn_model.pth')\n",
    "\n",
    "    # Modell nach dem Training laden und evaluieren\n",
    "    loaded_model = RNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "    loaded_model = load_model(loaded_model, 'rnn_model.pth')\n",
    "\n",
    "    # Testen und Evaluieren (optional, ein Testdatensatz muss vorhanden sein)\n",
    "    test_file_path = 'D:/Master_EI/FP/Modeling_Dynamic_Systems/DynSys_and_DataSets/lorenz_attractor_dataset_test.csv'  # Beispiel Testdatensatz\n",
    "    test_dataloader = create_dataloader(test_file_path, batch_size)\n",
    "    evaluate(loaded_model, test_dataloader, loss_fn)\n",
    "\n",
    "    print(\"Training abgeschlossen und Modell evaluiert.\")\"\"\"\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo\n"
     ]
    }
   ],
   "source": [
    "print(\"Hallo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
