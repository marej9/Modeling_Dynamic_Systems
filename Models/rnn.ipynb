{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cpu' device\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RNN character generator\n",
    "RNN implementation with Dense layers\n",
    "There is an RNN layer in pytorch, but in this case we will be using\n",
    "normal Dense layers to demonstrate the difference between\n",
    "RNN and Normal feedforward networks.\n",
    "This is a character level generator, which means it will create character by character\n",
    "You can input any text file and it will generate characters based on that text\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using '{device}' device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic RNN block. This represents a single layer of RNN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, batch_size: int) -> None:\n",
    "        \"\"\"\n",
    "        input_size: Number of features of your input vector\n",
    "        hidden_size: Number of hidden neurons\n",
    "        output_size: Number of features of your output vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.i2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns computed output and tanh(i2h + h2h)\n",
    "        Inputs\n",
    "        ------\n",
    "        x: Input vector\n",
    "        hidden_state: Previous hidden state\n",
    "        Outputs\n",
    "        -------\n",
    "        out: Linear output (without activation because of how pytorch works)\n",
    "        hidden_state: New hidden state matrix\n",
    "        \"\"\"\n",
    "        x = self.i2h(x)\n",
    "        hidden_state = self.h2h(hidden_state)\n",
    "        hidden_state = torch.tanh(x + hidden_state)\n",
    "        out = self.h2o(hidden_state)\n",
    "        return out, hidden_state\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def init_zero_hidden(self, batch_size=1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\t\t\t\tHelper function.\n",
    "        Returns a hidden state with specified batch size. Defaults to 1\n",
    "        \"\"\"\n",
    "        return torch.zeros(batch_size, self.hidden_size, requires_grad=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal use case\n",
    "\n",
    "N = 320 samples\n",
    "\n",
    "k = 3 features\n",
    "\n",
    "data matrix.shape = (320, 3) tuple\n",
    "\n",
    "sample = data_matrix[i,:].shape = 3 \n",
    "\n",
    "\n",
    "### using batch size\n",
    "\n",
    "N = 320 samples\n",
    "\n",
    "k = 3 features\n",
    "\n",
    "batch size = 32\n",
    "\n",
    "data_matrix.shape = (10, 32 , 3)\n",
    "\n",
    "batch = data_matrix[1,:,:].shape = 32, 3  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: RNN, data: DataLoader, epochs: int, optimizer: optim.Optimizer, loss_fn: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model for the specified number of epochs\n",
    "    Inputs\n",
    "    ------\n",
    "    model: RNN model to train\n",
    "    data: Iterable DataLoader\n",
    "    epochs: Number of epochs to train the model\n",
    "    optiimizer: Optimizer to use for each epoch\n",
    "    loss_fn: Function to calculate loss\n",
    "    \"\"\"\n",
    "    train_losses = {}\n",
    "    model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"=> Starting training\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = list()\n",
    "        for X, Y in data:\n",
    "            # skip batch if it doesnt match with the batch_size\n",
    "            if X.shape[0] != model.batch_size:   # x,y is batch\n",
    "                continue\n",
    "            hidden = model.init_zero_hidden(batch_size=model.batch_size)\n",
    "\n",
    "            # send tensors to device\n",
    "            X, Y, hidden = X.to(device), Y.to(device), hidden.to(device)\n",
    "\n",
    "            # 2. clear gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            \n",
    "            output = model(X)\n",
    "            loss = loss_fn(output, Y)\n",
    "\n",
    "            # 4. Compte gradients gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # 5. Adjust learnable parameters\n",
    "            # clip as well to avoid vanishing and exploding gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "            print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = open('datasets/Dinos/dinos.txt', 'r').read() # use any text file you want to learn\n",
    "  \n",
    "\n",
    "    # Data size variables\n",
    "    seq_length = 25\n",
    "    batch_size = 64\n",
    "    hidden_size = 256\n",
    "\n",
    "    text_dataset = TextDataset(data, seq_length=seq_length)\n",
    "    text_dataloader = DataLoader(text_dataset, batch_size)\n",
    "\n",
    "    # Model\n",
    "    rnnModel = RNN(1, hidden_size, len(text_dataset.chars)) # 1 because we enter a single number/letter per step.\n",
    "\n",
    "    # Train variables\n",
    "    epochs = 1000\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(rnnModel.parameters(), lr = 0.001)\n",
    "\n",
    "    train(rnnModel, text_dataloader, epochs, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "output = model(X)\n",
    "loss = loss_fn(output, Y)\n",
    "\n",
    "loss.backward()  # mse mean squared error, L2 norm\n",
    "\n",
    "\n",
    "optimizer.step()  # adam optimizer \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
